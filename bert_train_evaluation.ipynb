{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Config and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig,BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.num_labels = 28\n",
    "        self.epochs = 10\n",
    "        self.lr = 2e-5\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = 'bert-base-uncased'\n",
    "        self.result_path = 'result'\n",
    "        self.checkpoint_path = 'saved_dict'\n",
    "        self.batch_size = 32\n",
    "        self.max_length = 32\n",
    "        self.score_key = 'f1_score'\n",
    "\n",
    "config = Config()\n",
    "# 定义Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.model)\n",
    "\n",
    "# 自定义数据集类，用于PyTorch的DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length, num_class=28, mode='train'):\n",
    "        self.data = pd.read_csv(\"data/{}.tsv\".format(mode), sep='\\t', header=None)\n",
    "        self.data.columns = ['text', 'labels', 'id']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'][idx]\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        labels = self.data['labels'][idx].split(',')\n",
    "        return { \n",
    "            'input_ids': encoding['input_ids'].squeeze(0), \n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(convert_onehot(labels), dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "def convert_onehot(labels):\n",
    "    res = [0] * config.num_labels\n",
    "    for label in labels:\n",
    "        res[int(label)] = 1\n",
    "    return res\n",
    "\n",
    "train_dataset = TextDataset(tokenizer, max_length=config.max_length, mode='train')\n",
    "dev_dataset = TextDataset(tokenizer, max_length=config.max_length, mode='dev')\n",
    "test_dataset = TextDataset(tokenizer, max_length=config.max_length, mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def train(train_loader, val_loader, model, config):\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr)\n",
    "    \n",
    "    model_name = '{}_B-{}_E-{}_Lr-{}_add'.format(config.model, config.batch_size,\n",
    "                                                        config.epochs, config.lr)\n",
    "    max_score = 0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "\n",
    "        preds_all = np.zeros((0, config.num_labels), dtype=int)\n",
    "        labels_all = np.zeros((0, config.num_labels), dtype=int)\n",
    "        loss_all, length = 0, 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc='Training', colour='MAGENTA'):\n",
    "            labels = batch['labels'].int().numpy()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for key, val in batch.items():\n",
    "                batch[key] = val.to(config.device)\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            preds = get_preds(logits)\n",
    "            preds_all = np.concatenate((preds_all, preds))\n",
    "            labels_all = np.concatenate((labels_all, labels))\n",
    "            loss_all += loss.item()\n",
    "            length += 1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_score = get_scores(preds_all, labels_all, loss_all, length, 'train')\n",
    "        dev_score, _ =  evaluate(val_loader, model, config)\n",
    "        f = open('{}/{}.all_scores.txt'.format(config.result_path, model_name), 'a')\n",
    "        f.write(' ==================================================  Epoch: {}  ==================================================\\n'.format(epoch))\n",
    "        f.write('TrainScore: \\n{}\\nEvalScore: \\n{}\\n'.format(\n",
    "                json.dumps(train_score), json.dumps(dev_score)))\n",
    "        max_score = save_best(config, epoch, model_name,\n",
    "                                  model, dev_score, max_score)\n",
    "        print('End for {} epoch'.format(epoch))\n",
    "        \n",
    "def save_best(config, epoch, model_name, model, score, max_score):\n",
    "    score_key = config.score_key\n",
    "    curr_score = score[score_key]\n",
    "    print('The epoch_{} {}: {}\\nCurrent max {}: {}'.format(\n",
    "        epoch, score_key, curr_score, score_key, max_score))\n",
    "\n",
    "    if curr_score > max_score or epoch == 0:\n",
    "        torch.save({\n",
    "            'epoch': config.epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }, '{}/{}-BEST.tar'.format(config.checkpoint_path, model_name))\n",
    "        return curr_score\n",
    "    else:\n",
    "        return max_score\n",
    "\n",
    "def get_preds(logit):\n",
    "    return (logit > 0).int().cpu().numpy()\n",
    "\n",
    "def get_scores(preds, labels, loss_all=None, length=None, data_name='train'):\n",
    "    score_dict = dict()\n",
    "    union = preds | labels\n",
    "    inter = preds & labels\n",
    "    tp = np.sum(inter, axis=1)\n",
    "    accuracy =  tp / np.sum(union, axis=1)\n",
    "    precision = tp / (np.sum(preds, axis=1) + 1e-10) # 防止除0\n",
    "    recall = tp / np.sum(labels, axis=1)\n",
    "    f1_score = 2*(precision*recall)/(precision + recall + 1e-10)\n",
    "    \n",
    "    score_dict['accuracy'] = np.mean(accuracy)\n",
    "    score_dict['precision'] = np.mean(precision)\n",
    "    score_dict['recall'] = np.mean(recall)\n",
    "    score_dict['f1_score'] = np.mean(f1_score)\n",
    "    if loss_all is not None:\n",
    "        score_dict['all_loss'] = loss_all / length\n",
    "\n",
    "    print(\"Evaling on \\\"{}\\\" data\".format(data_name))\n",
    "    for s_name, s_val in score_dict.items():\n",
    "        print(\"{}: {}\".format(s_name, s_val))\n",
    "    return score_dict\n",
    "\n",
    "def evaluate(val_loader, model, config):\n",
    "    model.eval()\n",
    "    preds_all = np.zeros((0, config.num_labels), dtype=int)\n",
    "    labels_all = np.zeros((0, config.num_labels), dtype=int)\n",
    "    loss_all, length = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Evaling', colour='CYAN'):\n",
    "            labels = batch['labels'].int().numpy()\n",
    "            for key, val in batch.items():\n",
    "                batch[key] = val.to(config.device)\n",
    "            outputs = model(**batch)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            preds = get_preds(logits)\n",
    "\n",
    "            preds_all = np.concatenate((preds_all, preds))\n",
    "            labels_all = np.concatenate((labels_all, labels))\n",
    "            loss_all += loss.item()\n",
    "            length += 1\n",
    "    \n",
    "    dev_score = get_scores(preds_all, labels_all, loss_all, length, 'dev')\n",
    "    return dev_score, preds_all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:17<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.16295629166109565\n",
      "precision: 0.17390011095568528\n",
      "recall: 0.16738462719803426\n",
      "f1_score: 0.166980639720203\n",
      "all_loss: 0.15064760671438499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 43.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.34483446962717273\n",
      "precision: 0.3729500644550949\n",
      "recall: 0.34520299735888454\n",
      "f1_score: 0.3541305816241977\n",
      "all_loss: 0.10174957268378314\n",
      "The epoch_0 f1_score: 0.3541305816241977\n",
      "Current max f1_score: 0\n",
      "End for 0 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:19<00:00, 17.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.41066958458112574\n",
      "precision: 0.44487445284677835\n",
      "recall: 0.41193273439299694\n",
      "f1_score: 0.4222084004800003\n",
      "all_loss: 0.09398879052339534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 42.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.45130827344757696\n",
      "precision: 0.49004975119519845\n",
      "recall: 0.45457895706651924\n",
      "f1_score: 0.4650390024718762\n",
      "all_loss: 0.08856828659772872\n",
      "The epoch_1 f1_score: 0.4650390024718762\n",
      "Current max f1_score: 0.3541305816241977\n",
      "End for 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:20<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.522343929970053\n",
      "precision: 0.5664017506927211\n",
      "recall: 0.5273669661368348\n",
      "f1_score: 0.5384496659219061\n",
      "all_loss: 0.0790788012672718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 42.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.4790092746145814\n",
      "precision: 0.5203918677701805\n",
      "recall: 0.4855352865303114\n",
      "f1_score: 0.49479761680471285\n",
      "all_loss: 0.08640128450796884\n",
      "The epoch_2 f1_score: 0.49479761680471285\n",
      "Current max f1_score: 0.4650390024718762\n",
      "End for 2 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:21<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.6153359440989019\n",
      "precision: 0.6673270367154913\n",
      "recall: 0.6241323043845504\n",
      "f1_score: 0.6353513014801683\n",
      "all_loss: 0.06742131575535744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 42.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5405841164547632\n",
      "precision: 0.5848227995251423\n",
      "recall: 0.5517167250168908\n",
      "f1_score: 0.5589214421162294\n",
      "all_loss: 0.08911058405304656\n",
      "The epoch_3 f1_score: 0.5589214421162294\n",
      "Current max f1_score: 0.49479761680471285\n",
      "End for 3 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:21<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.6995868847423787\n",
      "precision: 0.7558723027748888\n",
      "recall: 0.7114892881824465\n",
      "f1_score: 0.7221414858706713\n",
      "all_loss: 0.05659970880932859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5070480928689884\n",
      "precision: 0.5520852526953847\n",
      "recall: 0.5278238437442417\n",
      "f1_score: 0.528984706046572\n",
      "all_loss: 0.09429361322785125\n",
      "The epoch_4 f1_score: 0.528984706046572\n",
      "Current max f1_score: 0.5589214421162294\n",
      "End for 4 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:22<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.7661053520694157\n",
      "precision: 0.8231359900911104\n",
      "recall: 0.7793680411579513\n",
      "f1_score: 0.7894357235315694\n",
      "all_loss: 0.04690261387218863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 42.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5310945273631841\n",
      "precision: 0.5764080829862115\n",
      "recall: 0.5549720533136785\n",
      "f1_score: 0.5542411399240513\n",
      "all_loss: 0.10037500207476756\n",
      "The epoch_5 f1_score: 0.5542411399240513\n",
      "Current max f1_score: 0.5589214421162294\n",
      "End for 5 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:22<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.8139345772863396\n",
      "precision: 0.8707690239500465\n",
      "recall: 0.8291703140597404\n",
      "f1_score: 0.8379450641440638\n",
      "all_loss: 0.03897100615650745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:04<00:00, 40.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5297893249800381\n",
      "precision: 0.5756403168786869\n",
      "recall: 0.5557705300657207\n",
      "f1_score: 0.5538541858057773\n",
      "all_loss: 0.10562693702385706\n",
      "The epoch_6 f1_score: 0.5538541858057773\n",
      "Current max f1_score: 0.5589214421162294\n",
      "End for 6 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:23<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.8508519542348153\n",
      "precision: 0.9047204944225166\n",
      "recall: 0.8662355064117331\n",
      "f1_score: 0.8739430786425552\n",
      "all_loss: 0.03249875659242481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:04<00:00, 42.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.533843130028868\n",
      "precision: 0.5794023708067003\n",
      "recall: 0.5684693814876236\n",
      "f1_score: 0.5607333701304005\n",
      "all_loss: 0.11245935250292806\n",
      "The epoch_7 f1_score: 0.5607333701304005\n",
      "Current max f1_score: 0.5589214421162294\n",
      "End for 7 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:23<00:00, 16.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.8800572064808416\n",
      "precision: 0.9308051139397304\n",
      "recall: 0.8951025109421791\n",
      "f1_score: 0.9021109355086073\n",
      "all_loss: 0.026930882796568297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:04<00:00, 42.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5234230084147166\n",
      "precision: 0.5675941280710808\n",
      "recall: 0.5598089797923961\n",
      "f1_score: 0.5506295681536136\n",
      "all_loss: 0.1200662418761674\n",
      "The epoch_8 f1_score: 0.5506295681536136\n",
      "Current max f1_score: 0.5607333701304005\n",
      "End for 8 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [01:23<00:00, 16.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.900697611917377\n",
      "precision: 0.9470840051311054\n",
      "recall: 0.9148782922521693\n",
      "f1_score: 0.9210825900522344\n",
      "all_loss: 0.022604327550735777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:04<00:00, 42.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5161507278422701\n",
      "precision: 0.5591947668535475\n",
      "recall: 0.5561697684417418\n",
      "f1_score: 0.5442601805244636\n",
      "all_loss: 0.12801834975971896\n",
      "The epoch_9 f1_score: 0.5442601805244636\n",
      "Current max f1_score: 0.5607333701304005\n",
      "End for 9 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config.model = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(config.model, num_labels=config.num_labels).to(config.device)\n",
    "train(train_loader, test_loader, model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [03:58<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.25424053763604365\n",
      "precision: 0.27473760199196123\n",
      "recall: 0.2596847884512017\n",
      "f1_score: 0.26174031054518354\n",
      "all_loss: 0.1293997049891694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:10<00:00, 15.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.4422486333763282\n",
      "precision: 0.4785332595801732\n",
      "recall: 0.4435999017259383\n",
      "f1_score: 0.4545543885056815\n",
      "all_loss: 0.09069331498707042\n",
      "The epoch_0 f1_score: 0.4545543885056815\n",
      "Current max f1_score: 0\n",
      "End for 0 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:09<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.48525109421792206\n",
      "precision: 0.5275205405298279\n",
      "recall: 0.4894743914612608\n",
      "f1_score: 0.5004760807302335\n",
      "all_loss: 0.08548167836286166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5075701738222468\n",
      "precision: 0.5495669798608577\n",
      "recall: 0.5121921257907991\n",
      "f1_score: 0.5228364350562644\n",
      "all_loss: 0.08456194291219991\n",
      "The epoch_1 f1_score: 0.5228364350562644\n",
      "Current max f1_score: 0.4545543885056815\n",
      "End for 1 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:12<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.5929870229593797\n",
      "precision: 0.6446018581873781\n",
      "recall: 0.6013994471319973\n",
      "f1_score: 0.6127096016326813\n",
      "all_loss: 0.07123435584055472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5355475707880352\n",
      "precision: 0.5810761009199051\n",
      "recall: 0.5491370308949083\n",
      "f1_score: 0.5550334745470974\n",
      "all_loss: 0.08714902153348221\n",
      "The epoch_2 f1_score: 0.5550334745470974\n",
      "Current max f1_score: 0.5228364350562644\n",
      "End for 2 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:15<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.7032262151577976\n",
      "precision: 0.761114950397788\n",
      "recall: 0.7155808953390156\n",
      "f1_score: 0.7264290649884915\n",
      "all_loss: 0.056467017647518596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5342577237270437\n",
      "precision: 0.5810146796318628\n",
      "recall: 0.5569989558380934\n",
      "f1_score: 0.5574841839626835\n",
      "all_loss: 0.0923270632238949\n",
      "The epoch_3 f1_score: 0.5574841839626835\n",
      "Current max f1_score: 0.5550334745470974\n",
      "End for 3 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:16<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.7920321738462719\n",
      "precision: 0.8516624432866153\n",
      "recall: 0.8069116179067803\n",
      "f1_score: 0.8166859730873784\n",
      "all_loss: 0.043200480098326194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.547616854001597\n",
      "precision: 0.5932989373545784\n",
      "recall: 0.5717093544622566\n",
      "f1_score: 0.5710337202306451\n",
      "all_loss: 0.1026441698346068\n",
      "The epoch_4 f1_score: 0.5710337202306451\n",
      "Current max f1_score: 0.5574841839626835\n",
      "End for 4 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:18<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.8525508715349766\n",
      "precision: 0.9094659447788539\n",
      "recall: 0.8676464716271213\n",
      "f1_score: 0.8765161087756855\n",
      "all_loss: 0.03292277273663657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5327375468337326\n",
      "precision: 0.5759167126779237\n",
      "recall: 0.5677783919906639\n",
      "f1_score: 0.5593022541056232\n",
      "all_loss: 0.1140287245021147\n",
      "The epoch_5 f1_score: 0.5593022541056232\n",
      "Current max f1_score: 0.5710337202306451\n",
      "End for 5 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:19<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.8867833832450279\n",
      "precision: 0.938969515382418\n",
      "recall: 0.9020221915073333\n",
      "f1_score: 0.90927501883297\n",
      "all_loss: 0.02623055455828732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5109698421472882\n",
      "precision: 0.5524230697842201\n",
      "recall: 0.553236901910202\n",
      "f1_score: 0.5394816042703762\n",
      "all_loss: 0.12521540890721714\n",
      "The epoch_6 f1_score: 0.5394816042703762\n",
      "Current max f1_score: 0.5710337202306451\n",
      "End for 6 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:21<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.9093143754212633\n",
      "precision: 0.95555756046404\n",
      "recall: 0.9230434615679951\n",
      "f1_score: 0.9294138574428513\n",
      "all_loss: 0.02132659053084061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.512146059824335\n",
      "precision: 0.5535286529791712\n",
      "recall: 0.5536207849640685\n",
      "f1_score: 0.5401142435431397\n",
      "all_loss: 0.1300198796479141\n",
      "The epoch_7 f1_score: 0.5401142435431397\n",
      "Current max f1_score: 0.5710337202306451\n",
      "End for 7 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:19<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.9254707056745758\n",
      "precision: 0.9659970819938573\n",
      "recall: 0.9384277816171388\n",
      "f1_score: 0.9434154955691967\n",
      "all_loss: 0.01734548890702399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:11<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5281739450893679\n",
      "precision: 0.5698882132021142\n",
      "recall: 0.5880474172348136\n",
      "f1_score: 0.5624101713094561\n",
      "all_loss: 0.13633491887765772\n",
      "The epoch_8 f1_score: 0.5624101713094561\n",
      "Current max f1_score: 0.5710337202306451\n",
      "End for 8 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|\u001b[35m██████████\u001b[0m| 1357/1357 [04:19<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"train\" data\n",
      "accuracy: 0.9400341703140597\n",
      "precision: 0.9747761651540955\n",
      "recall: 0.951535744452123\n",
      "f1_score: 0.9556017740572809\n",
      "all_loss: 0.014264276805906861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:10<00:00, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.5152939008660402\n",
      "precision: 0.556307966289559\n",
      "recall: 0.5706037712671213\n",
      "f1_score: 0.5476611650196318\n",
      "all_loss: 0.1433068057631745\n",
      "The epoch_9 f1_score: 0.5476611650196318\n",
      "Current max f1_score: 0.5710337202306451\n",
      "End for 9 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config.model = 'bert-large-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(config.model, num_labels=config.num_labels).to(config.device)\n",
    "train(train_loader, test_loader, model, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of bert-base-uncased after 10 epochs of fine_tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:03<00:00, 43.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.533843130028868\n",
      "precision: 0.5794023708067003\n",
      "recall: 0.5684693814876236\n",
      "f1_score: 0.5607333701304005\n",
      "all_loss: 0.11245935250292806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_data(model_path, test_loader, config):\n",
    "    model = BertForSequenceClassification.from_pretrained(config.model, num_labels=config.num_labels).to(config.device)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('metrics of {} after {} epochs of fine_tune'.format(config.model, checkpoint['epoch']))\n",
    "    test_score, preds = evaluate(test_loader, model, config)\n",
    "    return test_score, preds\n",
    "\n",
    "config.model = 'bert-base-uncased'\n",
    "model_path = \"saved_dict/bert-base-uncased_B-32_E-10_Lr-2e-05_add-BEST.tar\"\n",
    "test_score, preds = test_data(model_path, test_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of bert-large-uncased after 10 epochs of fine_tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaling: 100%|\u001b[36m██████████\u001b[0m| 170/170 [00:09<00:00, 18.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaling on \"dev\" data\n",
      "accuracy: 0.547616854001597\n",
      "precision: 0.5932989373545784\n",
      "recall: 0.5717093544622566\n",
      "f1_score: 0.5710337202306451\n",
      "all_loss: 0.1026441698346068\n"
     ]
    }
   ],
   "source": [
    "config.model = 'bert-large-uncased'\n",
    "model_path = \"saved_dict/bert-large-uncased_B-32_E-10_Lr-2e-05_add-BEST.tar\"\n",
    "test_score, preds = test_data(model_path, test_loader, config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics of llama_zero_shot\n",
      "Evaling on \"test\" data\n",
      "accuracy: 0.2096631042319268\n",
      "precision: 0.2348565812690553\n",
      "recall: 0.24006510656593577\n",
      "f1_score: 0.2266416301566222\n",
      "----------------------------------------------------------------------------------------------------\n",
      "metrics of llama_fine_tuned\n",
      "Evaling on \"test\" data\n",
      "accuracy: 0.34401493414760415\n",
      "precision: 0.36251765858661245\n",
      "recall: 0.41290461273877527\n",
      "f1_score: 0.36660261300023267\n"
     ]
    }
   ],
   "source": [
    "def analyze_llm(result_path, name):\n",
    "    llm_result = pd.read_csv(result_path)\n",
    "    preds = np.zeros((0, config.num_labels), dtype=int)\n",
    "    labels = np.zeros((0, config.num_labels), dtype=int)\n",
    "\n",
    "    for pred, label in zip(llm_result['predicted_sentiment'], llm_result['sentiment_index']):\n",
    "        pred = [json.loads(pred)]\n",
    "        preds = np.append(preds, pred, axis=0)\n",
    "        label = [convert_onehot(label.split(','))]\n",
    "        labels = np.append(labels, label, axis=0)\n",
    "    print('metrics of {}'.format(name))\n",
    "    get_scores(preds, labels, data_name='test')\n",
    "\n",
    "analyze_llm(\"result/goemotions_with_predictions_llama_3.1_8B_instruct.csv\", 'llama_zero_shot')\n",
    "print(\"-\"*100)\n",
    "analyze_llm(\"result/goemotions_with_predictions__llama_3.1_8B_instruct_sst_finetuned.csv\", 'llama_fine_tuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goemotions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
